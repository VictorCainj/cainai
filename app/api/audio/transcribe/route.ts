import { NextRequest, NextResponse } from 'next/server'
import OpenAI from 'openai'
import { TranscriptionResult } from '@/types/audio'

const openai = new OpenAI({
  apiKey: process.env.OPENAI_API_KEY!
})

export async function POST(request: NextRequest) {
  try {
    const formData = await request.formData()
    const audioFile = formData.get('audio') as File
    const language = formData.get('language') as string || 'pt'
    const autoSummarize = formData.get('autoSummarize') === 'true'

    if (!audioFile) {
      return NextResponse.json(
        { error: 'Arquivo de √°udio √© obrigat√≥rio' },
        { status: 400 }
      )
    }

    // Validar tipo de arquivo
    if (!audioFile.type.startsWith('audio/')) {
      return NextResponse.json(
        { error: 'Arquivo deve ser um √°udio v√°lido' },
        { status: 400 }
      )
    }

    // Validar tamanho (25MB m√°ximo)
    const maxSize = 25 * 1024 * 1024
    if (audioFile.size > maxSize) {
      return NextResponse.json(
        { error: 'Arquivo muito grande. M√°ximo: 25MB' },
        { status: 400 }
      )
    }

    console.log(`üéµ Iniciando transcri√ß√£o: ${audioFile.name} (${(audioFile.size / 1024 / 1024).toFixed(2)}MB)`)

    const startTime = Date.now()

    // Transcrever com OpenAI Whisper
    const transcription = await openai.audio.transcriptions.create({
      file: audioFile,
      model: 'whisper-1',
      language: language,
      response_format: 'verbose_json', // Para obter timestamps e metadados
      temperature: 0.2 // Mais determin√≠stico
    })

    const transcriptionTime = Date.now() - startTime

    // Extrair informa√ß√µes da transcri√ß√£o
    const fullText = transcription.text
    const duration = transcription.duration || 0
    const segments = transcription.segments || []

    console.log(`‚úÖ Transcri√ß√£o conclu√≠da em ${transcriptionTime}ms - ${fullText.length} caracteres`)

    // Gerar resumo automaticamente se solicitado
    let summary = ''
    let summaryTime = 0

    if (autoSummarize && fullText.length > 100) {
      const summaryStartTime = Date.now()
      
      try {
        const summaryResponse = await openai.chat.completions.create({
          model: 'gpt-4o-mini',
          messages: [
            {
              role: 'system',
              content: `Voc√™ √© um assistente especializado em resumir √°udios transcritos. 
              Crie um resumo conciso e informativo em portugu√™s do √°udio a seguir.
              O resumo deve:
              - Ter no m√°ximo 200 palavras
              - Destacar os pontos principais
              - Manter o tom e contexto original
              - Ser claro e objetivo`
            },
            {
              role: 'user',
              content: `Transcri√ß√£o do √°udio:\n\n${fullText}`
            }
          ],
          temperature: 0.3,
          max_tokens: 300
        })

        summary = summaryResponse.choices[0].message.content || 'N√£o foi poss√≠vel gerar resumo'
        summaryTime = Date.now() - summaryStartTime
        
        console.log(`üìù Resumo gerado em ${summaryTime}ms`)
      } catch (error) {
        console.error('‚ùå Erro ao gerar resumo:', error)
        summary = 'Erro ao gerar resumo autom√°tico'
      }
    } else {
      // Resumo simples para textos curtos
      summary = fullText.length > 200 
        ? fullText.substring(0, 200) + '...'
        : fullText
    }

    // Calcular confian√ßa m√©dia dos segments
    let averageConfidence = 0.8 // Default se n√£o houver segments
    if (segments && segments.length > 0) {
      const confidences = segments
        .map((s: any) => s.avg_logprob || 0)
        .filter(conf => conf !== 0)
      
      if (confidences.length > 0) {
        // Converter logprob para confian√ßa (aproxima√ß√£o)
        const avgLogProb = confidences.reduce((sum, conf) => sum + conf, 0) / confidences.length
        averageConfidence = Math.max(0, Math.min(1, Math.exp(avgLogProb)))
      }
    }

    // Preparar resposta
    const result: TranscriptionResult = {
      id: `transcription_${Date.now()}`,
      text: fullText,
      summary: summary,
      confidence: averageConfidence,
      duration: duration,
      timestamp: new Date(),
      language: transcription.language || language,
      segments: segments.map((segment: any) => ({
        start: segment.start,
        end: segment.end,
        text: segment.text,
        confidence: Math.exp(segment.avg_logprob || -1)
      }))
    }

    const totalTime = Date.now() - startTime

    console.log(`üéØ Processamento completo em ${totalTime}ms`)

    return NextResponse.json(result, {
      headers: {
        'X-Transcription-Time': transcriptionTime.toString(),
        'X-Summary-Time': summaryTime.toString(),
        'X-Total-Time': totalTime.toString(),
        'X-Text-Length': fullText.length.toString(),
        'X-Audio-Duration': duration.toString()
      }
    })

  } catch (error) {
    console.error('üéµ Erro na transcri√ß√£o:', error)
    
    // Tratamento espec√≠fico de erros da OpenAI
    if (error instanceof Error) {
      if (error.message.includes('audio_too_long')) {
        return NextResponse.json(
          { error: '√Åudio muito longo. M√°ximo: 30 minutos' },
          { status: 400 }
        )
      }
      
      if (error.message.includes('rate_limit')) {
        return NextResponse.json(
          { error: 'Limite de uso atingido. Tente novamente em alguns minutos' },
          { status: 429 }
        )
      }
      
      if (error.message.includes('invalid_file_format')) {
        return NextResponse.json(
          { error: 'Formato de arquivo n√£o suportado' },
          { status: 400 }
        )
      }
    }
    
    return NextResponse.json(
      { 
        error: 'Erro interno na transcri√ß√£o',
        details: process.env.NODE_ENV === 'development' ? 
          (error instanceof Error ? error.message : 'Erro desconhecido') : 
          'Tente novamente em alguns momentos'
      },
      { status: 500 }
    )
  }
} 